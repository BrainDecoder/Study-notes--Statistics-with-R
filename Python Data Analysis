#IMPORT/EXPORT

# Import libraries
import pandas as pd
import numpy as np
# Read the file
path = "https://.../auto.csv"
df = pd.read_csv(path, header=None)
# Show the first 5 rows
df.head(5)
# Add headers
headers = ["symboling","normalized-losses","...","make","fuel-type"]
df.columns = headers
# Save file
df.to_csv("automobile.csv", index=False)
# Learn the type of each attribute
df.dtypes
# Statistical summary of numerical columns
df.describe()
# Statistical summary of all column, including categorical
df.describe(include = "all")
df[['length', 'compression-ratio']].describe() #specific columns
# Info about the index dtype and columns, non-null values, memory usage
df.info()

# DATA WRANGLING

# Replace "?" with NaN and replace the old dataframe
df.replace("?", np.nan, inplace = True)
# Detecting missing data
missing_data = df.isnull() # creates a boolean data frame with True for NAN
missing_data.head(5)
# Calculate the number of missing values in each column
for column in missing_data.columns.values.tolist():
    print(column)
    print (missing_data[column].value_counts())
    print("")
# Replace "NaN" by mean value
avg_norm_loss = df["losses"].astype("float").mean(axis=0)
df["losses"].replace(np.nan, avg_norm_loss, inplace=True)
# See which values are present in a particular column
df['num-of-doors'].value_counts()
# Calculate the most common type automatically
df['num-of-doors'].value_counts().idxmax()
# drop the rows with NaN in "price" column
df.dropna(subset=["price"], axis=0, inplace=True)
# reset index, because rows were droped
df.reset_index(drop=True, inplace=True)

# Convert data types to proper format
df[["bore", "stroke"]] = df[["bore", "stroke"]].astype("float")
# Standardization - Transform mpg to L/100km
df['city-L/100km'] = 235/df["city-mpg"]
# Normalization, Simple feature scaling: x_new = x_old / x_max
df['length'] = df['length']/df['length'].max()
# Min-Max: x_new = (x_old - x_min)/(x_max - x_min)
df['length'] = (df['length']-df['length'].min())/(df['length'].max()-df['length'].min())
# Z-score: x_new = (x_old - mean(x))/sd(x)
df['length'] = (df['length']-df['length'].mean())/df['length'].std()

# Creating Bins - linspace(start_value, end_value, numbers_generated)
bins = np.linspace(min(df["hp"]), max(df["hp"]), 4)
group_names = ['Low', 'Medium', 'High']
df['hp-binned'] = pd.cut(df['hp'], bins, labels=group_names, include_lowest=True )
# Creating indicator variables (or dummy variables), a must for regression
dummy_var = pd.get_dummies(df["fuel-type"])
# Rename columns to meaningful ones
dummy_var.rename(columns={'old_name':'new_name', 'old_name': 'new_name'}, inplace=True)
# Merge data frame "df" and "dummy_var" 
df = pd.concat([df, dummy_var], axis=1)
# Drop original column "fuel-type" from "df"
df.drop("fuel-type", axis = 1, inplace=True)

#EXPLORATORY DATA ANALYSIS

# Calculate the correlation between variables of type "int64" or "float64"
df.corr()
df[['bore', 'stroke', 'ratio', 'hp']].corr()
# Understanding the (linear) relationship
import matplotlib.pyplot as plt
import seaborn as sns
sns.regplot(x="engine-size", y="price", data=df)
plt.ylim(0,)
# Understanding categorical variables 
sns.boxplot(x="body-style", y="price", data=df)

# Descriptive Statistical Analysis
df.describe()
df.describe(include=['object']) #for categorical data
df['drive-wheels'].value_counts()
drive_wheels_counts = df['drive-wheels'].value_counts().to_frame() #nicely represents as a table
drive_wheels_counts.rename(columns={'drive-wheels': 'value_counts'}, inplace=True)
drive_wheels_counts.index.name = 'drive-wheels' #makes the table readable

# Grouping
df['drive-wheels'].unique() #shows unique values
df_group_one = df[['drive-wheels','price']]
df_group_one = df_group_one.groupby(['drive-wheels'],as_index=False).mean()
# Group with multiple variables
df_group_two = df[['drive-wheels','body-style','price']]
grouped_test1 = df_group_two.groupby(['drive-wheels','body-style'],as_index=False).mean()
# Make easier to visualize by creating a pivot table
grouped_pivot = grouped_test1.pivot(index='drive-wheels',columns='body-style')
grouped_pivot = grouped_pivot.fillna(0) #fill missing values with 0

# Use a heat map to visualize the relationship
plt.pcolor(grouped_pivot, cmap='RdBu')
plt.colorbar()
plt.show()
# Making the viz beautiful:
fig, ax = plt.subplots()
im = ax.pcolor(grouped_pivot, cmap='RdBu')
# Label names
row_labels = grouped_pivot.columns.levels[1]
col_labels = grouped_pivot.index
# Move ticks and labels to the center
ax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)
ax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)
# Insert labels
ax.set_xticklabels(row_labels, minor=False)
ax.set_yticklabels(col_labels, minor=False)
# Rotate label if too long
plt.xticks(rotation=90)
fig.colorbar(im)
plt.show()

# Correlation and Causation
from scipy import stats
pearson_coef, p_value = stats.pearsonr(df['wheel-base'], df['price'])
print("The Pearson Correlation Coefficient is", pearson_coef, " with a P-value of P =", p_value)  

# ANOVA: Analysis of Variance or difference between groups of the same variable
grouped_test2=df_group_two[['drive-wheels', 'price']].groupby(['drive-wheels'])
# Obtain the values of the method group
grouped_test2.get_group('4wd')['price']
# 'f_oneway' in the module 'stats' obtains the F-test score and P-value
f_val, p_val = stats.f_oneway(grouped_test2.get_group('fwd')['price'], grouped_test2.get_group('rwd')['price'], grouped_test2.get_group('4wd')['price'])  
print( "ANOVA results: F=", f_val, ", P =", p_val)
# High F and low P-value doesn't mean all three tested groups are correlated between them
# Separate, similar comparison between 2 is needed for all variables

#MODEL DEVELOPMENT

# Simple Linear Regression
from sklearn.linear_model import LinearRegression
lm = LinearRegression() #create a lin. reg. object
X = df[['highway-mpg']] #predictor
Y = df[['price']] #response
lm.fit(X,Y) #fit the model
Yhat=lm.predict(X) #output a prediction
Yhat[0:5]
lm.intercept_ # shows the intercept
lm.coef_ #shows the slope

# Multiple Linear Regression
Z = df[['horsepower', 'weight', 'engine', 'mpg']] #predictors
lm.fit(Z, df['price'])
lm.intercept_
lm.coef_

# Model Evaluation using Visualization

# Regression Plot = scattered data points + LR line
import seaborn as sns
width = 12 #of the graph itself
height = 10 #of the graph itself
plt.figure(figsize=(width, height))
sns.regplot(x="highway-mpg", y="price", data=df)
plt.ylim(0,)

# Residual Plot = residuals on y-axis & independent variable on x-axis
width = 12 #of the graph itself
height = 10 #of the graph itself
plt.figure(figsize=(width, height))
sns.residplot(df['highway-mpg'], df['price'])
plt.show() #if points randomly spread around x-axis -> LR model is good

# Distribution Plot (to visualize Multiple LR model)
Y_hat = lm.predict(Z) #make prediction
plt.figure(figsize=(width, height))
ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value") #actuals
sns.distplot(Y_hat, hist=False, color="b", label="Fitted Values" , ax=ax1) # predicted
plt.title('Actual vs Fitted Values for Price') #elements of the graph
plt.xlabel('Price (in dollars)')
plt.ylabel('Proportion of Cars')
plt.show()
plt.close()
#end of Evaluation using Visualization

# Polynomial Regression
# Build function to plot the data
def PlotPolly(model, independent_variable, dependent_variabble, Name):
    x_new = np.linspace(15, 55, 100) 
    y_new = model(x_new) 
    plt.plot(independent_variable, dependent_variabble, '.', x_new, y_new, '-')
    plt.title('Polynomial Fit with Matplotlib for Price ~ Length')
    ax = plt.gca()
    ax.set_facecolor((0.898, 0.898, 0.898))
    fig = plt.gcf()
    plt.xlabel(Name)
    plt.ylabel('Price of Cars')
    plt.show()
    plt.close()
# Get the variables
x = df['highway-mpg']
y = df['price']
# Fit the polynomial
f = np.polyfit(x, y, 3) #polynomial of the 3rd order (cubic)
# Display the polynomial function
p = np.poly1d(f)
print(p)
# Plot the function
PlotPolly(p, x, y, 'highway-mpg')
np.polyfit(x, y, 3) #show the model details

# Polynomial transform on multiple features
from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=2) #create a PolynomialFeatures object of degree 2
Z_pr=pr.fit_transform(Z)
Z.shape #shape of original data
Z_pr.shape #shape of transformed data

# Pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
# Create a list of tuples (name of model/estimator and its corresp. constructor)
Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model',LinearRegression())]
# Input the list as an argument to the pipeline constructor
pipe=Pipeline(Input)
pipe #show details
# Normalize the data, perform a transform and fit the model simultaneously = run the pipeline
pipe.fit(Z,y)
# Normalize, transform and produce a prediction simultaneously
ypipe=pipe.predict(Z)
ypipe[0:4]

# R-squared for SLR
lm.fit(X, Y)
print('The R-square is: ', lm.score(X, Y))
# MSE for SLR
from sklearn.metrics import mean_squared_error
Yhat=lm.predict(X) #Yhat is predicted var, X is the input variable
mse = mean_squared_error(df['price'], Yhat)
print('The mean square error of price and predicted value is: ', mse)
# R-squared for MLR
lm.fit(Z, df['price'])
print('The R-square is: ', lm.score(Z, df['price']))
# MSE for MLR
Y_predict_multifit = lm.predict(Z)
mse_m = mean_squared_error(df['price'], Y_predict_multifit)
print('The MSE of price and predicted value using multifit is: ', mse_m)
# R-squared for Polynomial LR
from sklearn.metrics import r2_score
r_squared = r2_score(y, p(x))
print('The R-square value is: ', r_squared)
# MSE for Polynomial LR
mean_squared_error(df['price'], p(x))

# Predictions
new_input=np.arange(1, 100, 1).reshape(-1, 1) #new input
lm.fit(X, Y) #fit model
yhat=lm.predict(new_input) #produce a prediction
yhat[0:5]
plt.plot(new_input, yhat) #plot data
plt.show()

# MODEL EVALUATION AND REFINEMENT

df=df._get_numeric_data() #use only numeric data
from IPython.html import widgets #libraries for plotting
from IPython.display import display
from ipywidgets import interact, interactive, fixed, interact_manual
# Functions for plotting
def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))
    ax1 = sns.distplot(RedFunction, hist=False, color="r", label=RedName)
    ax2 = sns.distplot(BlueFunction, hist=False, color="b", label=BlueName, ax=ax1)
    plt.title(Title)
    plt.xlabel('Price (in dollars)')
    plt.ylabel('Proportion of Cars')
    plt.show()
    plt.close()
def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):
    width = 12
    height = 10
    plt.figure(figsize=(width, height))
    #training data 
    #testing data 
    # lr:  linear regression object 
    #poly_transform:  polynomial transformation object 
    xmax=max([xtrain.values.max(), xtest.values.max()])
    xmin=min([xtrain.values.min(), xtest.values.min()])
    x=np.arange(xmin, xmax, 0.1)
    plt.plot(xtrain, y_train, 'ro', label='Training Data')
    plt.plot(xtest, y_test, 'go', label='Test Data')
    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')
    plt.ylim([-10000, 60000])
    plt.ylabel('Price')
    plt.legend()

# Training and testing
y_data = df['price'] #place the target data in a separate dataframe
x_data=df.drop('price',axis=1) #drop price data in x
# Randomly split data into training and testing data
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)
print("number of test samples :", x_test.shape[0])
print("number of training samples:",x_train.shape[0])
# Train the model
from sklearn.linear_model import LinearRegression
lre=LinearRegression()
lre.fit(x_train[['horsepower']], y_train)
lre.score(x_test[['horsepower']], y_test) #R^2 on the test data
lre.score(x_train[['horsepower']], y_train) #R^2 on the train data

# Cross-validation Score
from sklearn.model_selection import cross_val_score
Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)
Rcross #show details
print("The mean of the folds are", Rcross.mean(), "and the standard deviation is" , Rcross.std())
# Use negative squared error as a score
-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')
# cross_val_predict
from sklearn.model_selection import cross_val_predict
yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)
yhat[0:5]

# Overfitting, Underfitting and Model Selection
lr = LinearRegression()
lr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)
yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat_train[0:5]
yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])
yhat_test[0:5]

Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'
DistributionPlot(y_train, yhat_train, "Actual Values (Train)", "Predicted Values (Train)", Title)
Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'
DistributionPlot(y_test,yhat_test,"Actual Values (Test)","Predicted Values (Test)",Title)

# Overfitting
from sklearn.preprocessing import PolynomialFeatures
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)
pr = PolynomialFeatures(degree=5) #perform a degree 5 polynomial transformation
x_train_pr = pr.fit_transform(x_train[['horsepower']])
x_test_pr = pr.fit_transform(x_test[['horsepower']])
pr
poly = LinearRegression() #create a linear regression model and train it
poly.fit(x_train_pr, y_train)
yhat = poly.predict(x_test_pr)
yhat[0:5]
print("Predicted values:", yhat[0:4]) #compare first five predicted values to actual targets
print("True values:", y_test[0:4].values)
# Display training data
PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)
poly.score(x_train_pr, y_train) #R^2 of the training data
poly.score(x_test_pr, y_test) #R^2 of the test data

# Show how the R^2 changes on the test data for different order polynomials
Rsqu_test = []
order = [1, 2, 3, 4]
for n in order:
    pr = PolynomialFeatures(degree=n)
    x_train_pr = pr.fit_transform(x_train[['horsepower']])
    x_test_pr = pr.fit_transform(x_test[['horsepower']])    
    lr.fit(x_train_pr, y_train)
    Rsqu_test.append(lr.score(x_test_pr, y_test))
plt.plot(order, Rsqu_test)
plt.xlabel('order')
plt.ylabel('R^2')
plt.title('R^2 Using Test Data')
plt.text(3, 0.75, 'Maximum R^2 ')  
def f(order, test_data):
    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)
    pr = PolynomialFeatures(degree=order)
    x_train_pr = pr.fit_transform(x_train[['horsepower']])
    x_test_pr = pr.fit_transform(x_test[['horsepower']])
    poly = LinearRegression()
    poly.fit(x_train_pr,y_train)
    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)
interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))

# Ridge regression
pr=PolynomialFeatures(degree=2) #an example of two degree polynomial transformation
x_train_pr=pr.fit_transform(x_train[['horsepower', '...','symboling']])
x_test_pr=pr.fit_transform(x_test[['horsepower', '...','symboling']])
from sklearn.linear_model import Ridge #import Ridge
RigeModel=Ridge(alpha=0.1) #create a Ridge regression object, setting the regularization parameter to 0.1
RigeModel.fit(x_train_pr, y_train) # Like regular regression, fit the model
yhat = RigeModel.predict(x_test_pr) #obtain a prediction
print('predicted:', yhat[0:4]) #compare first 5 predicted samples with test set
print('test set :', y_test[0:4].values)
#Select the value of Alfa that minimizes the test error
Rsqu_test = []
Rsqu_train = []
dummy1 = []
ALFA = 10 * np.array(range(0,1000))
for alfa in ALFA:
    RigeModel = Ridge(alpha=alfa) 
    RigeModel.fit(x_train_pr, y_train)
    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))
    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))
# Plot out the value of R^2 for different Alphas
width = 12
height = 10
plt.figure(figsize=(width, height))
plt.plot(ALFA,Rsqu_test, label='validation data  ')
plt.plot(ALFA,Rsqu_train, 'r', label='training Data ')
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.legend()

# Grid Search - class GridSearchCV makes easier finding the best Alpha hyperparameter
from sklearn.model_selection import GridSearchCV
# Create a dictionary of parameter values
parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]
# Create a ridge regions object
RR=Ridge()
RR
# Create a ridge grid search object
Grid1 = GridSearchCV(RR, parameters1,cv=4)
# Fit the model
Grid1.fit(x_data[['horsepower', '...', 'highway-mpg']], y_data)
# Obtain the estimator with the best parameters
BestRR=Grid1.best_estimator_
BestRR
# Test the model on the test data
BestRR.score(x_test[['horsepower', '...', 'highway-mpg']], y_test)
